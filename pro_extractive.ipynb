{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nlist_dir = []\nfor dirname, _, filenames in os.walk('/kaggle/input/ulasan'):\n    for filename in filenames:\n        ld = os.path.join(dirname, filename)\n        list_dir.append(ld)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\ndef get_files(list_dir):\n\n    df_list = pd.DataFrame(columns=[\"review\",\"extractive\",\"abstractive\"])\n    review = []\n    extractive = []\n    abstractive = []\n    for f in list_dir:\n        print('file ' + str(f))\n        data = pd.read_csv(f, encoding=\"ISO-8859-1\", sep=',', usecols=['review','extractive', 'abstractive'])\n        data_length = len(data)\n        data = data.astype(str)\n\n        str_rev = \"\"\n        str_ext = \"\"\n        for d in range(data_length):\n            str_rev += \" \" + data['review'][d]\n            str_ext += \" \" + data['extractive'][d]\n      \n        review.append(str_rev)\n        extractive.append(str_ext)\n        abstractive.append(data['abstractive'][0])\n        \n    df_list['review'] = review\n    df_list['extractive'] = extractive\n    df_list['abstractive'] = abstractive\n\n    print(len(df_list))\n    return df_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = get_files(list_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install Sastrawi\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nimport itertools\nnltk.download('stopwords')\n!pip install langdetect\n!pip install google_trans_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stop_id = StopWordRemoverFactory().get_stop_words()\nstop_factory = stopwords.words('indonesian')\n# more_stopword = ['daring', 'online', 'pd']\n\n\n# https://en.wikipedia.org/wiki/Unicode_block\nEMOJI_PATTERN = re.compile(\n    \"[\"\n    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n    \"\\U0001F600-\\U0001F64F\"  # emoticons\n    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n    \"\\U00002702-\\U000027B0\"  # Dingbats\n    \"]+\"\n)\n\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\n#stopword\ndef get_file(file):\n    stopwords=[]\n    file_stopwords = open(file,'r')\n    row = file_stopwords.readline()\n    while row:\n        word = row.strip()\n        stopwords.append(word)\n        row = file_stopwords.readline()\n    file_stopwords.close()\n    return stopwords\n\ndef first_process(text):\n    text = text.lower()                               # change to lower case\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # delete URL\n    text = re.sub(r'[-+]?[0-9]+', '', text)           # delete number\n    text = re.sub(r'[^\\w\\s\\.]',' ', text)        # delete punctuation\n    text = re.sub(r'  ',' ', text)                    # delete more space\n    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n    text = text.strip()\n    return text\n\n#normalize text\ndef text_normalize(key_file, text):\n    key_norm = pd.read_csv(key_file, index_col='_id')\n    text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])\n    text = str.lower(text)\n    return text\n\n#stopword removal\ndef remove_stop_words(text):\n    # sw = stop_factory + more_stopword\n    sw = stop_factory\n    clean_words = []\n    text = text.split()\n    for word in text:\n        if word not in sw:\n            clean_words.append(word)\n    return \" \".join(clean_words)\n\ndef remove_nan(text):\n  clean_words = []\n  text = text.split()\n  for word in text:\n    if str(word) != 'nan':\n      clean_words.append(word)\n  return \" \".join(clean_words)\n\n#remove emoji\ndef emoji(text):\n    return re.sub(EMOJI_PATTERN, r\"\", text)\n\n#delete duplicate char\ndef duplicate_char(text, key_word):\n    text = text.split()\n    clean_words = []\n    for word in text:\n        text_s = stemmer.stem(word)\n        tmp = []\n        if text_s not in key_word:\n            tmp = ''.join(ch for ch, _ in itertools.groupby(word))\n            clean_words.append(tmp)\n        else:\n            clean_words.append(word)\n    return \" \".join(clean_words)\n    \n# all process\ndef clean_process(text):\n    text = first_process(text)\n    # text = text_normalize('/content/gdrive/MyDrive/Colab Notebooks/Thesis/data/key_norm_new.csv',text)\n    # text = remove_stop_words(text)\n    text = remove_nan(text)\n    text = emoji(text)\n    basic_text = get_file('/kaggle/input/corpus/prepro/kata-dasar.txt')\n    text = duplicate_char(text,basic_text)\n    return text\n    \ndata['review_cln'] = data['review'].apply(clean_process)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['review_cln'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['extractive_cln'] = data['extractive'].apply(clean_process)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom tqdm import tqdm\n\n\ndef split_sent(data):\n    sentence_delimiters = re.compile(u'[.\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s')\n\n    df_as = []\n\n    for i in tqdm(data):\n      x = \"\"\n      x = sentence_delimiters.split(i)\n      x = [sen.rstrip().lstrip() for sen in x if sen and not sen.isspace()]\n      df_as.append(x)\n    \n    return df_as","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_as = split_sent(data['review_cln'])\ndata['sent_tok'] = df_as","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data['sent_tok'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_ex = split_sent(data['extractive_cln'])\n# data['ex_tok'] = df_ex","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\nfrom transformers import BertTokenizer, BertModel\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\nmodel = BertModel.from_pretrained('indobenchmark/indobert-base-p1', output_hidden_states = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_gpu = False # Use GPU for text vectorization\nn_combined_hidden_states = 1 # Number of last hidden states for combining to single vector\npooling_mode = 0 # pooling strategy for BERT-embeddings\n\nif use_gpu:\n    model.to('cuda')\n\ndef sentence2bert_hidden_states(sentence, tokenizer, model, use_gpu = True):\n    indexed_tokens = tokenizer.encode(sentence)\n    tokens_tensor = torch.tensor([indexed_tokens])\n    if use_gpu:\n        tokens_tensor = tokens_tensor.to('cuda')\n    with torch.no_grad():\n        outputs = model(tokens_tensor)        \n    hidden_states = outputs[2]\n    hidden_states = torch.stack(hidden_states, dim=0)\n    hidden_states = torch.squeeze(hidden_states, dim=1)\n    hidden_states = hidden_states.permute(1,0,2)    \n    return hidden_states, indexed_tokens\n\ndef combine_hidden_states(hidden_states, n_summands):\n    n = hidden_states.shape[0]    \n    assert n_summands <= hidden_states.shape[1]    \n    combined = np.empty((hidden_states.shape[0],hidden_states.shape[2]))\n    for i in range(n):\n        combined[i,:] = torch.sum(hidden_states[i][-n_summands:], dim=0).cpu()[:]\n    return combined\n\ndef sentence2token_embeddings(sentence, tokenizer, model, use_gpu = True, n_combined_hidden_states = 4):\n    hidden_states, indexed_tokens = sentence2bert_hidden_states(sentence, tokenizer, model, use_gpu)\n    token_embeddings = combine_hidden_states(hidden_states, n_combined_hidden_states)\n    return token_embeddings, indexed_tokens\n\n# Sentence vectorization\ndef sentence2embedding(sentence, tokenizer, model, use_gpu = True, n_combined_hidden_states = 4, pooling_mode = 0):\n    token_embeddings, indexed_tokens = sentence2token_embeddings(sentence, tokenizer, model, use_gpu, n_combined_hidden_states)\n    if pooling_mode == 0:\n        return token_embeddings[0]\n    else:\n        return np.mean(token_embeddings, axis = 0)\n\n# Splitting text into sentences and sentence by sentence vectorization\ndef text2sentence_embeddings(text, tokenizer, model, use_gpu = True, n_combined_hidden_states = 4, pooling_mode = 0):\n    text_sentences = text#nltk.sent_\n    n_text_sentences = len(text_sentences)\n    text_embeddings = np.empty((n_text_sentences, model.config.hidden_size))\n    for i in tqdm(range(n_text_sentences)):\n        text_embeddings[i,:] = sentence2embedding(text_sentences[i], tokenizer, model, use_gpu, n_combined_hidden_states, pooling_mode)[:]\n    return text_sentences, text_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef sentence_embeddings(data):\n  text_sentences, text_embeddings = text2sentence_embeddings(data, \n                                                            tokenizer, \n                                                            model, \n                                                            use_gpu, \n                                                            n_combined_hidden_states, \n                                                            pooling_mode)\n  return text_sentences, text_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine(u, v):\n  return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scikit-learn-extra","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install yellowbrick","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef gap_statistic(embedding, k_max, nrefs=10):\n  gaps = np.zeros((len(range(2, k_max)),))\n  data_max = np.amax(embedding[:,1])\n  data_min = np.amin(embedding[:,1])\n\n  for gap_index, k in enumerate(range(2, k_max)):\n    refDisps = np.zeros(nrefs)\n\n    for i in range(nrefs):\n      reference = np.random.uniform(data_min, data_max, embedding.shape[0]).reshape(-1,1)\n      kmed_model = KMedoids(k)\n      kmed_model.fit(reference)\n      refDisp = kmed_model.inertia_\n      refDisps[i] = refDisp\n    \n    kmed_model = KMedoids(k)\n    kmed_model.fit(embedding)\n        \n    origDisp = kmed_model.inertia_\n\n    gap = np.log(np.mean(refDisps)) - np.log(origDisp)\n    gaps[gap_index] = gap\n\n  return (gaps.argmax() + 2)\n\n# text, embedding = sentence_embeddings(df['sent_tok'][0])\n# pca = PCA(n_components=2)\n# embedding = pca.fit_transform(embedding)\n# s = gap_statistic(embedding, 20)\n# print(s)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\ndef get_silhouette (embedding, k_max):\n  silhouette = {}\n  for n_cluster in range(2, k_max):\n    silhouette[n_cluster] = silhouette_score(embedding, KMedoids(n_clusters = n_cluster).fit_predict(embedding))\n  \n  index_max = max(silhouette, key=silhouette.get)\n  return index_max","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yellowbrick.cluster import KElbowVisualizer\n\ndef elbow(embedding, k_max):\n  kmed_model = KMedoids()\n  elb_visualizer = KElbowVisualizer(kmed_model, k=(2,k_max), timings=False)\n  elb_visualizer.fit(embedding)\n  index_clus = elb_visualizer.elbow_value_\n\n  return index_clus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn_extra.cluster import KMedoids\n\n\ndef n_clustering(embedding, k_max):\n    pca = PCA(n_components=2)\n    embedding = pca.fit_transform(embedding)\n    clus = []\n\n  # for n_cluster in range(2, len(embedding)):\n  #   silhouette_scores[n_cluster] = silhouette_score(embedding, KMedoids(n_clusters = n_cluster).fit_predict(embedding))\n  #   # silhouette_scores.append(silhouette_score(embedding,\n  #   #                                           AgglomerativeClustering(n_clusters = n_cluster).fit_predict(embedding))) \n  \n  # index_max = max(silhouette_scores, key=silhouette_scores.get)\n#   k_max = 10\n#   print(k_max)\n    gs = gap_statistic(embedding, k_max)\n    ss = get_silhouette(embedding, k_max)\n    el = elbow(embedding, k_max)\n    \n    clus = [gs, ss, el]\n    \n    for cl in clus:\n        if cl is None:\n            clus.remove(cl)\n            \n    print(clus)\n    clus.sort()\n    clus_opt = {x:clus.count(x) for x in clus}\n    index_max = max(clus_opt, key=clus_opt.get)\n    print(index_max)\n\n    return index_max\n\ndef clustering(embedding,n_cluster):\n    if n_cluster >= 1:\n        n_cluster = np.round(n_cluster)\n    else:\n        n_cluster = 1\n\n    agg = KMedoids(n_clusters=n_cluster)\n    agg.fit(embedding)\n\n    return agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\ndef clustering_sent(data, cls_max=False):\n    data_ext = defaultdict(dict)\n    count = 0\n\n#     for i in data:\n#       text, embedding = sentence_embeddings(i)\n#       tmp_txt = []\n#       tmp_clus = []\n#       tmp_centroid = []\n#       if len(embedding) <= 6:\n#         for i in range(len(text)):\n#           tmp_txt.append(text[i])\n#           tmp_clus.append(0)\n#       else:\n#         cls = n_clustering(embedding)\n#         agg = clustering(embedding,cls)\n#         for i in range(cls):\n#           tmp_centroid.append(agg.cluster_centers_[i])\n#         for i in range(len(text)):\n#           tmp_txt.append(text[i])\n#           tmp_clus.append(agg.labels_[i])\n#       data_ext[count]['text'] = tmp_txt\n#       data_ext[count]['cluster'] = tmp_clus\n#       data_ext[count]['centroid'] = tmp_centroid\n#       count += 1\n        \n    for i in data:\n        text, embedding = sentence_embeddings(i)\n        tmp_txt = []\n        tmp_clus = []\n        tmp_centroid = []\n        \n        if cls_max == False:\n            k_max = 10\n        else:\n            k_max = len(i)\n        \n        cls = n_clustering(embedding, k_max)\n        agg = clustering(embedding,cls)\n\n        for i in range(len(text)):\n            tmp_txt.append(text[i])\n            tmp_clus.append(agg.labels_[i])\n            \n        data_ext[count]['text'] = tmp_txt\n        data_ext[count]['cluster'] = tmp_clus\n        count += 1\n    \n    return data_ext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ext = clustering_sent(data['sent_tok'], cls_max=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_ext)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk import sent_tokenize\n\n\ndef convert_to_embeddings(text_sentences, text_embeddings, text, lexicon, keyword):\n  # text = \" \".join(data)\n  # print(text)\n  hypothesis_embeddings = {}\n  sum = 0\n  index = 0\n\n  # num_clusters = 6\n  # clustering_model = KMeans(n_clusters=num_clusters)\n  # clustering_model.fit(text_embeddings)\n  # cluster_assignment = clustering_model.labels_\n\n  # clustered_sentences = [[] for i in range(num_clusters)]\n  # for sentence_id, cluster_id in enumerate(cluster_assignment):\n  #     clustered_sentences[cluster_id].append(df['sent_tok'][0][sentence_id])\n  for sent in text_sentences:\n    var = text_embeddings[index]\n    sum += var\n    hypothesis_embeddings[sent] = var\n    index += 1\n\n  centroid = sum/len(text_sentences)\n  \n  #print(\"done\")\n  #print(hypothesis_embeddings)\n  content_relevance_score = content_relevance(centroid,hypothesis_embeddings)\n  sentence_novelty_score = sentence_novelty(content_relevance_score,hypothesis_embeddings)\n  # sentence_position_score = sentence_position(hypothesis)\n  sentiment = softmax_sen(text, lexicon, keyword)\n  return total_score(content_relevance_score, sentence_novelty_score, sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def content_relevance(centroid,hypothesis_embeddings):\n  d = {}\n  for sentence,embed in hypothesis_embeddings.items():\n    d[sentence] = cosine(centroid,embed)\n    \n  return d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_novelty(content_relevance_score,hypothesis_embeddings):\n  novel_sentences = {}\n  TAU = 0.70\n  for sent1,embed1 in hypothesis_embeddings.items():\n    max_similarity = 0\n    for sent2,embed2 in hypothesis_embeddings.items():\n      if sent1!=sent2 and cosine(embed1,embed2)>max_similarity:\n          max_similarity = cosine(embed1,embed2)\n\n    if max_similarity<TAU:\n      novel_sentences[sent1] = 1\n\n    elif max_similarity>TAU and content_relevance_score[sent1]>content_relevance_score[sent2]:\n      novel_sentences[sent1] = 1\n\n    else:\n      novel_sentences[sent1] = 1-max_similarity\n\n  return novel_sentences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/LIAAD/yake","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def key_clean_process(text):\n    text = remove_stop_words(text)\n    return text\n\ndata['review_key'] = data['review'].apply(key_clean_process)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['review_key'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yake\n\ndef keyword_extractor(text):\n    kword = []\n    kw_extractor = yake.KeywordExtractor()\n    keywords = kw_extractor.extract_keywords(text)\n    \n    for kw in range(len(keywords)):\n        if len(keywords[kw][0].split()) >= 2:\n            kword.append(keywords[kw])\n    \n    keywords = sorted(kword,key=lambda x:(-x[1],x[0]))\n\n    return keywords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyword = []\nfor i in data['review_key']:\n  keyword.append(keyword_extractor(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(data)):\n  data_ext[i]['keyword'] = keyword[i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ext[0]['keyword']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import exp\n\n# Fungsi Load Lexicon\ndef loadLexicon(file):\n    df=open(file,\"r\",encoding=\"utf-8\", errors='replace')\n    data=df.readlines();df.close()\n    return [d.strip().lower() for d in data]\n\n# fpos, fneg, fnegasi = '/content/gdrive/MyDrive/dataset/sentimen/positive.tsv', '/content/gdrive/MyDrive/dataset/sentimen/negative.tsv', '/content/gdrive/MyDrive/dataset/sentimen/s-negasi.txt'\n# # positif, negatif, negasi = loadLexicon(fpos), loadLexicon(fneg), loadLexicon(fnegasi)\n# pos_lexicon = pd.read_csv('lexicon/InSet-master/positive.tsv',sep='\\t')\n# neg_lexicon = pd.read_csv('lexicon/InSet-master/negative.tsv',sep='\\t')\n# lexicon = pos_lexicon.append(neg_lexicon,ignore_index=True)\n\n \n# calculate the softmax of a vector\ndef softmax(vector):\n\te = exp(vector)\n\treturn e / e.sum()\n\ndef prediksiSentiment(kalimat, lexicon, keyword, index):\n    # Naive Approach, nanti akan kita diskusikan bagaimana improvisasi fungsi sederhana ini\n    # posWords = []\n    # negWords = [w for w in negatif if w in kalimat]\n    # for w in positif:\n    #     if w in kalimat:\n    #         negated = True\n    #         for n in negasi:\n    #             if n+' '+w in kalimat:\n    #                 negWords.append(n+' '+w)\n    #                 negated = True\n    #                 break\n    #         if not negated:\n    #             posWords.append(w)\n    # nPos, nNeg = len(posWords), len(negWords)\n    # print('posWords' + str(nPos))\n    # print('negWords' + str(nNeg))\n\n    add_weight = []\n    for i in range(0,len(lexicon)):\n      if lexicon['word'][i] in kalimat:\n        add_weight.append(lexicon['weight'][i])\n    val = sum(add_weight)\n    wordlist = len(kalimat.split())\n\n    nk = 0\n    for k in range(len(keyword)):\n      if keyword[k][0] in kalimat:\n        nk += keyword[k][1]\n        \n\n    return val, wordlist, nk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def softmax_sen(text, lexicon, keyword):\n  lprob = []\n  clus_sent = {}\n  index = 0\n  for i,sent in enumerate(text):\n    val, wordlist, key = prediksiSentiment(sent, lexicon, keyword, i)\n    try:\n      ns = (val / wordlist)\n    except:\n      ns = 0\n    prob = ns + key\n    lprob.append(prob)\n    clus_sent[sent] = val\n    index += 1\n  \n  sofProb = softmax(lprob)\n  score_sent = {}\n  for i,sent in enumerate(text):\n    score_sent[sent] = sofProb[i]\n    # print(sofProb[i],sent)\n  \n  return score_sent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def total_score(content_relevance_score, sentence_novelty_score, sentiment):\n  ALPHA = 0.45\n  BETA = 0.35\n  GAMMA = 0.20\n  final_score = {}\n  for sent in content_relevance_score:\n    final_score[sent] = ALPHA*content_relevance_score[sent]+BETA*sentence_novelty_score[sent]+GAMMA*sentiment[sent]\n    print(final_score[sent])\n  final_score = {k: v for k, v in sorted(final_score.items(), key=lambda item: item[1],reverse=True)}\n\n#   summary = \"\"\n  \n#   index = 0\n#   if len(final_score) <= 5:\n#     for sent in final_score:\n#       if index == (len(final_score)-1):\n#         summary += sent + \".\"\n#       else :\n#         summary += sent + \". \"\n#       index += 1\n#   else:\n#     for sent in final_score:\n#       if final_score[sent] >= threshold: #making summary of approx half length.\n#         if index == (len(final_score)-1):\n#           summary += sent + \".\"\n#         else :\n#           summary += sent + \". \"\n#       index += 1\n\n\n  # if len(final_score) > 1:\n  #   index = 0\n\n  #     if index == 0:\n  #       summary += sent\n  #     else:\n  #       summary += '. ' + sent\n  #   # for i in enumerate(final_score):\n  #   #   print(i)\n  #     index += 1\n  # else:\n  #   for sent in final_score:\n  #     summary = sent\n  # contra = False\n  # if len(final_score)>1:\n  #   clus = []\n  #   for i,sent in enumerate(final_score):\n  #     clus.append(cluster[sent])\n  #     # print(clus[i])\n\n  #   for i in range(1):\n  #     # if clus[i] == 0:\n  #     #   break\n  #     if clus[i] != clus[i+1]:\n  #       # if clus[i+1] == 0:\n  #       #   break\n  #       contra = True\n      \n  return final_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iter_ext(data, lexicon):\n    extractive = []\n    index = 0\n    \n    result_dict = defaultdict(list)\n\n    while index < len(data):\n        total = 0\n        print('data ke ' + str(index))\n        e_clus = defaultdict(list)\n        total = np.array(data[index]['cluster'])\n\n        datapoint_length = {x:data[index]['cluster'].count(x) for x in data[index]['cluster']}\n        dat_list = list(datapoint_length.values())\n\n        for c in range(len(datapoint_length)):\n            print('cluster ' + str(c))\n            txt = []\n\n            for t in range (len(data[index]['text'])):\n                if data[index]['cluster'][t] == c:\n                    txt.append(data[index]['text'][t])\n        \n      \n            print(len(txt))\n            text_sentences, text_embeddings = sentence_embeddings(txt)\n            predicted_summary = convert_to_embeddings(text_sentences, text_embeddings, data[index]['text'], lexicon, data[index]['keyword']) \n            e_clus[c].append(predicted_summary)\n        \n        result_dict[index].append(e_clus)\n#         output = \"\"\n#         for v in e_clus:\n#             output += v \n#         extractive.append(output)\n        index += 1\n\n    return result_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpos, fneg, fnegasi = '/kaggle/input/corpus/sentimen/positive.tsv', '/kaggle/input/corpus/sentimen/negative.tsv', '/kaggle/input/corpus/sentimen/s-negasi.txt'\n# positif, negatif, negasi = loadLexicon(fpos), loadLexicon(fneg), loadLexicon(fnegasi)\npos_lexicon = pd.read_csv(fpos,sep='\\t')\nneg_lexicon = pd.read_csv(fneg,sep='\\t')\nlexicon = pos_lexicon.append(neg_lexicon,ignore_index=True)\n\nextract = iter_ext(data_ext, lexicon)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summary(text, threshold):\n    extractive = []\n    for dc in range(len(text)):\n        tmp_ext = ''\n        for cl in range(len(text[dc][0])):\n            val_cls = [v for v in text[dc][0][cl][0].values()]\n            key_cls = [k for k in text[dc][0][cl][0].keys()]\n            for vc in range(len(val_cls)):\n                if val_cls[vc] >= threshold:\n                    if len(tmp_ext) == 0:\n                        tmp_ext += key_cls[vc] + '.'\n                    else:\n                        tmp_ext += ' ' + key_cls[vc] + '.'\n        extractive.append(tmp_ext)\n    \n    return extractive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\n#rouge metric based on https://pypi.org/project/rouge/\ndef my_rouge2(hypothesis, reference):\n    rouge = Rouge()\n    scores = rouge.get_scores(hypothesis, reference)\n    \n    #rouge-1\n    r1_f1=scores[0]['rouge-1']['f']\n    r1_precision=scores[0]['rouge-1']['p']\n    r1_recall=scores[0]['rouge-1']['r']\n    \n    #rouge-2\n    r2_f1=scores[0]['rouge-2']['f']\n    r2_precision=scores[0]['rouge-2']['p']\n    r2_recall=scores[0]['rouge-2']['r']\n    \n    return r1_f1#, r2_f1, #, precision, recall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"step = 0.5\nresult_treshold = {}\nwhile step <= 0.95:\n    print(\"=\"*5 + str(step) + \"=\"*5)\n    val_th = []\n    \n    extractive = summary(extract, step)\n\n    for i in range(len(extractive)):\n        if len(extractive[i].split()) != 0:\n            tmp_val = my_rouge2(extractive[i], data['abstractive'][i])\n            val_th.append(tmp_val)\n        else:\n            tmp_val = 0\n            val_th.append(tmp_val)\n  \n    tot = sum(val_th)/len(val_th)\n    result_treshold[step] = tot\n\n    step += 0.01\n    step = round(step,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_treshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_summary = summary(extract, 0.77)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['proposed_extractive'] = final_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.to_csv(f\"/kaggle/working/extractive.csv\", index=False, encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BERT2GPT2","metadata":{}},{"cell_type":"code","source":"import string\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nimport unicodedata\n\n# Remove puncuation from word\ndef rm_punc_from_word(word):\n    clean_alphabet_list = [\n        alphabet for alphabet in word if alphabet not in string.punctuation\n    ]\n    return ''.join(clean_alphabet_list)\n\n# Remove puncuation from text\ndef rm_punc_from_text(text):\n    clean_word_list = [rm_punc_from_word(word) for word in text]\n    return ''.join(clean_word_list)\n\n# Remove numbers from text\ndef rm_number_from_text(text):\n    text = re.sub('[0-9]+', '', text)\n    return ' '.join(text.split())  # to rm `extra` white space\n\n# Remove stopwords from text\ndef rm_stopwords_from_text(text):\n    _stopwords = stopwords.words('indonesian')\n    text = text.split()\n    word_list = [word for word in text if word not in _stopwords]\n    return ' '.join(word_list)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:27.163964Z","iopub.execute_input":"2022-11-26T03:12:27.164854Z","iopub.status.idle":"2022-11-26T03:12:27.174527Z","shell.execute_reply.started":"2022-11-26T03:12:27.164806Z","shell.execute_reply":"2022-11-26T03:12:27.173434Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cleaning text\ndef clean_text(text):\n    text = text.lower()\n    # text = rm_punc_from_text(text)\n    text = rm_number_from_text(text)\n    # text = rm_stopwords_from_text(text)\n\n    # there are hyphen(–) in many titles, so replacing it with empty str\n    # this hyphen(–) is different from normal hyphen(-)\n    text = re.sub('–', '', text)\n    text = ' '.join(text.split())  # removing `extra` white spaces\n\n    # Removing unnecessary characters from text\n    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n\n    # remove accented chars ('Sómě Áccěntěd těxt' => 'Some Accented text')\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\n        'utf-8', 'ignore'\n    )\n\n    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n\n    text = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n\n    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',\n                  str(text)).lower()\n\n    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    try:\n        url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n        repl_url = url.group(3)\n        text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n    except Exception as e:\n        pass\n\n    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:27.932629Z","iopub.execute_input":"2022-11-26T03:12:27.933010Z","iopub.status.idle":"2022-11-26T03:12:27.946930Z","shell.execute_reply.started":"2022-11-26T03:12:27.932976Z","shell.execute_reply":"2022-11-26T03:12:27.945858Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('/kaggle/input/extractive/extractive.csv')\ndata['abstractive'] = data['abstractive'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:28.883160Z","iopub.execute_input":"2022-11-26T03:12:28.883560Z","iopub.status.idle":"2022-11-26T03:12:28.952043Z","shell.execute_reply.started":"2022-11-26T03:12:28.883529Z","shell.execute_reply":"2022-11-26T03:12:28.951114Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:29.699027Z","iopub.execute_input":"2022-11-26T03:12:29.699804Z","iopub.status.idle":"2022-11-26T03:12:29.713528Z","shell.execute_reply.started":"2022-11-26T03:12:29.699733Z","shell.execute_reply":"2022-11-26T03:12:29.712497Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\nfrom datasets import Dataset\ntrain_data = Dataset.from_pandas(train_df)\nval_data = Dataset.from_pandas(val_df)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:30.418079Z","iopub.execute_input":"2022-11-26T03:12:30.418449Z","iopub.status.idle":"2022-11-26T03:12:41.636088Z","shell.execute_reply.started":"2022-11-26T03:12:30.418418Z","shell.execute_reply":"2022-11-26T03:12:41.634935Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.13.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.12)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers\nfrom transformers import BertTokenizer, GPT2Tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:13:09.040210Z","iopub.execute_input":"2022-11-26T03:13:09.040618Z","iopub.status.idle":"2022-11-26T03:13:18.914666Z","shell.execute_reply.started":"2022-11-26T03:13:09.040580Z","shell.execute_reply":"2022-11-26T03:13:18.911936Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.2.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.53)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mVersionConflict\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2350225182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .file_utils import (\n\u001b[1;32m     45\u001b[0m     \u001b[0m_BaseLazyModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\"\" require_version wrapper which emits a core-specific hint on failure \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgot_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwant_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         raise pkg_resources.VersionConflict(\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;34mf\"{requirement} is required for a normal functioning of this module, but found {pkg}=={got_ver}.{hint}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         )\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mVersionConflict\u001b[0m: tokenizers==0.9.4 is required for a normal functioning of this module, but found tokenizers==0.12.1.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master"],"ename":"VersionConflict","evalue":"tokenizers==0.9.4 is required for a normal functioning of this module, but found tokenizers==0.12.1.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master","output_type":"error"}]},{"cell_type":"code","source":"tokenizer_src = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\ntokenizer_src.bos_token = tokenizer_src.cls_token\ntokenizer_src.eos_token = tokenizer_src.sep_token","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:13:18.917746Z","iopub.status.idle":"2022-11-26T03:13:18.920179Z","shell.execute_reply.started":"2022-11-26T03:13:18.919932Z","shell.execute_reply":"2022-11-26T03:13:18.919958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make sure GPT2 appends EOS in begin and end\ndef build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.014474Z","iopub.status.idle":"2022-11-26T03:12:58.016242Z","shell.execute_reply.started":"2022-11-26T03:12:58.015985Z","shell.execute_reply":"2022-11-26T03:12:58.016010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\ntokenizer_tgt = GPT2Tokenizer.from_pretrained(\"cahya/gpt2-small-indonesian-522M\")\ntokenizer_tgt.pad_token = tokenizer_tgt.unk_token","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.017547Z","iopub.status.idle":"2022-11-26T03:12:58.018528Z","shell.execute_reply.started":"2022-11-26T03:12:58.018248Z","shell.execute_reply":"2022-11-26T03:12:58.018278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntext_word_count = []\nsummary_word_count = []\n\n# populate the lists with sentence lengths\nfor i in data['proposed_extractive']:\n      text_word_count.append(len(i.split()))\n\nfor i in data['abstractive']:\n      summary_word_count.append(len(i.split()))\n\nlength_df = pd.DataFrame({'ulasan':text_word_count, 'ringkasan':summary_word_count})\n\nlength_df.hist(bins = 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.020806Z","iopub.status.idle":"2022-11-26T03:12:58.021968Z","shell.execute_reply.started":"2022-11-26T03:12:58.021676Z","shell.execute_reply":"2022-11-26T03:12:58.021700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_ulasan = max(text_word_count)\nprint(max_ulasan)\nmax_ringkasan = max(summary_word_count)\nprint(max_ringkasan)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.023249Z","iopub.status.idle":"2022-11-26T03:12:58.024091Z","shell.execute_reply.started":"2022-11-26T03:12:58.023820Z","shell.execute_reply":"2022-11-26T03:12:58.023844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_max_length = max_ulasan\ndecoder_max_length = max_ringkasan\n\ndef process_data_to_model_inputs(batch):\n  # tokenize the inputs and labels\n  inputs = tokenizer_src(batch[\"proposed_extractive\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n  outputs = tokenizer_tgt(batch[\"abstractive\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n\n  batch[\"input_ids\"] = inputs.input_ids\n  batch[\"attention_mask\"] = inputs.attention_mask\n  batch[\"decoder_input_ids\"] = outputs.input_ids\n  batch[\"labels\"] = outputs.input_ids.copy()\n  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n\n  batch[\"labels\"] = [\n        [-100 if mask == 0 else token for mask, token in mask_and_tokens] for mask_and_tokens in [zip(masks, labels) for masks, labels in zip(batch[\"decoder_attention_mask\"], batch[\"labels\"])]\n    ]\n  \n  assert all([len(x) == encoder_max_length for x in inputs.input_ids])\n  assert all([len(x) == decoder_max_length for x in outputs.input_ids])\n  return batch","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.025531Z","iopub.status.idle":"2022-11-26T03:12:58.026099Z","shell.execute_reply.started":"2022-11-26T03:12:58.025833Z","shell.execute_reply":"2022-11-26T03:12:58.025858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32\n\ntrain_data = train_data.map(\n    process_data_to_model_inputs, \n    batched=True, \n    batch_size=batch_size, \n    remove_columns=[\"review\", \"extractive\", \"review_cln\", \"extractive_cln\", \"sent_tok\", \"review_key\"]\n)\nval_data = val_data.map(\n    process_data_to_model_inputs, \n    batched=True, \n    batch_size=batch_size, \n    remove_columns=[\"review\", \"extractive\", \"review_cln\", \"extractive_cln\", \"sent_tok\", \"review_key\"]\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.028010Z","iopub.status.idle":"2022-11-26T03:12:58.028481Z","shell.execute_reply.started":"2022-11-26T03:12:58.028240Z","shell.execute_reply":"2022-11-26T03:12:58.028263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.set_format(\n    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n)\n\nval_data.set_format(\n    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.030782Z","iopub.status.idle":"2022-11-26T03:12:58.031774Z","shell.execute_reply.started":"2022-11-26T03:12:58.031497Z","shell.execute_reply":"2022-11-26T03:12:58.031522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.033268Z","iopub.status.idle":"2022-11-26T03:12:58.033818Z","shell.execute_reply.started":"2022-11-26T03:12:58.033560Z","shell.execute_reply":"2022-11-26T03:12:58.033584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import EncoderDecoderModel\n\nbert2gpt = EncoderDecoderModel.from_encoder_decoder_pretrained(\"indobenchmark/indobert-base-p1\",\"cahya/gpt2-small-indonesian-522M\")","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.035732Z","iopub.status.idle":"2022-11-26T03:12:58.036238Z","shell.execute_reply.started":"2022-11-26T03:12:58.035991Z","shell.execute_reply":"2022-11-26T03:12:58.036014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert2gpt.config.decoder_start_token_id = tokenizer_tgt.bos_token_id\nbert2gpt.config.eos_token_id = tokenizer_tgt.eos_token_id\nbert2gpt.config.pad_token_id = tokenizer_tgt.pad_token_id\nbert2gpt.config.max_length = 20\nbert2gpt.config.min_length = 3\nbert2gpt.config.no_repeat_ngram_size = 3\nbert2gpt.early_stopping = True\nbert2gpt.length_penalty = 2.0\nbert2gpt.num_beams = 8","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.037504Z","iopub.status.idle":"2022-11-26T03:12:58.043798Z","shell.execute_reply.started":"2022-11-26T03:12:58.043540Z","shell.execute_reply":"2022-11-26T03:12:58.043566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score\nfrom datasets import load_metric\n\nrouge = load_metric(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.045187Z","iopub.status.idle":"2022-11-26T03:12:58.045973Z","shell.execute_reply.started":"2022-11-26T03:12:58.045699Z","shell.execute_reply":"2022-11-26T03:12:58.045724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    #preds, labels = eval_preds\n    # if isinstance(pred_ids, tuple):\n    #     pred_ids = pred_ids[0]\n    # for pred in pred_ids:\n    #     print(\"PRED BEFORE DECODING:\", pred)\n\n    # all unnecessary tokens are removed\n    pred_str = tokenizer_tgt.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer_tgt.eos_token_id\n    label_str = tokenizer_tgt.batch_decode(labels_ids, skip_special_tokens=True)\n    # decoded_preds, decoded_labels = postprocess_text(pred_str, label_str)\n\n    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n\n    return {\n        \"rouge2_precision\": round(rouge_output.precision, 4),\n        \"rouge2_recall\": round(rouge_output.recall, 4),\n        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n    }","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.047351Z","iopub.status.idle":"2022-11-26T03:12:58.048124Z","shell.execute_reply.started":"2022-11-26T03:12:58.047856Z","shell.execute_reply":"2022-11-26T03:12:58.047896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    # fp16=True, \n    output_dir=\"./\",\n    do_train=True,\n    do_eval=True,\n    logging_steps=50,  \n    save_steps=100, \n    # warmup_steps=50,  \n    # max_steps=1500, # delete for full training\n    num_train_epochs = 64, #TRAIN_EPOCHS\n    # save_total_limit=1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.049489Z","iopub.status.idle":"2022-11-26T03:12:58.050256Z","shell.execute_reply.started":"2022-11-26T03:12:58.049995Z","shell.execute_reply":"2022-11-26T03:12:58.050020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=bert2gpt,\n    tokenizer=tokenizer_tgt,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-11-26T03:12:58.051642Z","iopub.status.idle":"2022-11-26T03:12:58.052432Z","shell.execute_reply.started":"2022-11-26T03:12:58.052162Z","shell.execute_reply":"2022-11-26T03:12:58.052187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/bert2gpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EncoderDecoderModel.from_pretrained('/kaggle/working/bert2gpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = Dataset.from_pandas(test_df)\n\nbatch_size = 16  # change to 64 for full evaluation\n\n# map data correctly\ndef generate_summary(batch):\n    # Tokenizer will automatically set [BOS] <text> [EOS]\n    # cut off at BERT max length 512\n    inputs = tokenizer_src(batch[\"proposed_extractive\"], padding=\"max_length\", truncation=True, max_length=70, return_tensors=\"pt\")\n    input_ids = inputs.input_ids\n    attention_mask = inputs.attention_mask\n\n    outputs = model.generate(input_ids, attention_mask=attention_mask, num_beams=10,\n                                repetition_penalty=5.0, num_return_sequences=1)\n\n    # all special tokens including will be removed\n    output_str = tokenizer_tgt.batch_decode(outputs, skip_special_tokens=True)\n\n    batch[\"pred\"] = output_str\n\n    return batch\n\nresults = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"review\", \"extractive\", \"review_cln\", \"extractive_cln\", \"sent_tok\", \"review_key\"])\n\npred_str = results[\"pred\"]\nlabel_str = results[\"abstractive\"]\n\nrouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(test_data)):\n    print('ulasan: ',test_data[i]['proposed_extractive'])\n    print('ringkasan: ',label_str[i])\n    print('prediksi: ',pred_str[i])\n    print(\"===================\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}