{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7f5d5b2f935f4d8f97264486cc373b87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_123efcf00bfc44539a96aa7334f01261","IPY_MODEL_64ee34cc50ed4d39a2b971d412b37d7c","IPY_MODEL_02684394f3e24cf6903568c90f48b143"],"layout":"IPY_MODEL_c7d7cccbf0704da499ef5c14cabe6e6e"}},"123efcf00bfc44539a96aa7334f01261":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40ec8507f67545d3a28822babfaa5b3f","placeholder":"â€‹","style":"IPY_MODEL_0645d6614f2b4e1daf6c0d20527265fd","value":"Prediction: 100%"}},"64ee34cc50ed4d39a2b971d412b37d7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3adbd3567936459cba25549f2585bd06","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_406ae25ca3d4448592d8862caf82227e","value":16}},"02684394f3e24cf6903568c90f48b143":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d3fc48319054a16a613cae52f4cdb8f","placeholder":"â€‹","style":"IPY_MODEL_6012c5ec4ef3422db1438995281d757f","value":" 16/16 [00:03&lt;00:00,  5.82it/s]"}},"c7d7cccbf0704da499ef5c14cabe6e6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40ec8507f67545d3a28822babfaa5b3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0645d6614f2b4e1daf6c0d20527265fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3adbd3567936459cba25549f2585bd06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"406ae25ca3d4448592d8862caf82227e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d3fc48319054a16a613cae52f4cdb8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6012c5ec4ef3422db1438995281d757f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"AH9NNS6hU5KI"},"source":["**\"Huggingface is the Open AI we deseve\"**\n","\n","When I read that tweet, I could not have agreed more. \n","\n","Huggingface (ðŸ¤—) has democratized large-scale neural network models like no others have done. We can easily build on top of them and follow the fast-pace development of NLP neural networks. \n","\n","While there are plenty of tutorials already, I want to show an example of fine-tuning the multilingual BERT model for Indonesian dataset. In particular, I am using [AiryRooms Hotel Review](https://github.com/jordhy97/final_project) dataset for Aspect-Based Sentiment Analysis (ABSA). With just 2 fine-tuning epochs, we can achieve state-of-the-art aspect-sentiment extraction.\n","\n","This notebook aims to answer the following questions:\n","1. How do we prepare a custom ABSA dataset?\n","1. How to fine-tune multilingual BERT model?\n","2. How do we calculate F1 score as a custom metric?\n","3. How to use tensorboard to display training result?\n","4. How to make an inference of a sample sentence?\n","\n","Compared to my [previous post](https://medium.com/@yoseflaw/step-by-step-ner-model-for-bahasa-indonesia-with-pytorch-and-torchtext-6f94fca08406?source=friends_link&sk=c15c89082c00c8785577e1cebb77c9c2), this post is more concise. In essence, the length difference implies how fine-tuning is a powerful approach for building NLP models. Although I am using a model pretrained from 104 languages, the fine-tuned model can perform well for Bahasa Indonesia. The result has removed my doubt on using pretrained multilingual model for just one language. In fact, the hotel review dataset is also an informal one, and the model can still predict aspects and sentiments accurately!\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"MwAqCZyqg8Cv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3f6487c-a345-4f0f-eefd-11d4431c86ae","executionInfo":{"status":"ok","timestamp":1664355352774,"user_tz":-420,"elapsed":23357,"user":{"displayName":"Narandha R","userId":"05066405186273252932"}}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"RkzEtyiZiC3U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9cdf045a-c55d-4236-c50a-feb403cb6528"},"source":["!pip install transformers==3.0.2\n","\n","import torch\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from torch.utils.data import Dataset\n","from transformers import (BertTokenizerFast, BertForTokenClassification,\n","                          Trainer, TrainingArguments)\n","import numpy as np\n","from sklearn.metrics import f1_score, classification_report\n","from pathlib import Path\n","import os\n","import csv\n","import re\n","\n","nltk.download(\"punkt\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.0.2\n","  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 769 kB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 55.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880 kB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n","Collecting tokenizers==0.8.1.rc1\n","  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0 MB 43.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=f00bae6f895d35f762be604064811a732e17eb44c572e93ec0b487f5649418ec\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n"]}]},{"cell_type":"code","metadata":{"id":"pmDc9_PVisoh"},"source":["import sys\n","DRIVE_ROOT = \"/content/gdrive/MyDrive/Colab Notebooks/Thesis/Absa\"\n","if DRIVE_ROOT not in sys.path:\n","    sys.path.append(DRIVE_ROOT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZHNIvJrzl8T"},"source":["available_gpu = torch.cuda.is_available()\n","if available_gpu:\n","    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n","    use_device = torch.device(\"cuda\")\n","else:\n","    use_device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eU9mQrKUzmun"},"source":["# Dataset\n","\n","First, I begin with reading a custom PyTorch `Dataset` class. There are two main concepts here: how to use `tokenizer` and the `encode_tags` function. The former is provided by ðŸ¤—. Since we have to use the same tokenizer for all datasets, I leave the tokenizer instantiation at `Main`. We just need to make sure to provide the correct tokenization parameters according to the custom dataset. In this case, the dataset has been pretokenized, so I put `is_pretokenized` as `True`.\n","\n","The second concept is handling subwords with `encode_tags()`. The code snippet is implemented based on the ðŸ¤— tutorial on named entity recognition with BERT. Why do subwords matter? The challenge is: wh\u0010ich label should we apply to the subwords? In this implementation, we assign `-100` as the label, which is a code that those labels should be ignored during loss calculation. Therefore, the label predictions of the subwords do not matter for training. We will take only the first subword's label for every word."]},{"cell_type":"code","metadata":{"id":"TO5Q5C__iWSy"},"source":["class ReviewDataset(Dataset):\n","    def __init__(self, filepath, tokenizer, tag2idx=None):\n","        self.texts, self.tags = ReviewDataset.read_input(filepath)\n","        self.encodings = tokenizer(\n","            self.texts,\n","            is_pretokenized=True,  # skip word tokenization\n","            return_offsets_mapping=True,  # offsets are used in tag encoding\n","            padding=True,  # pad to max length\n","            truncation=True  # if longer than max length, truncate sentence\n","        )\n","        # make sure that the tag-to-idx dictionary is the same between train, val, and test\n","        if tag2idx is None:\n","            unique_tags = set(tag for doc in self.tags for tag in doc)\n","            self.num_labels = len(unique_tags)\n","            self.tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n","        else:\n","            self.tag2idx = tag2idx\n","        self.idx2tag = {idx: tag for tag, idx in self.tag2idx.items()}\n","        # tag encoding to handle subwords\n","        self.labels = self.encode_tags()\n","        self.encodings.pop(\"offset_mapping\")\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def encode_tags(self):\n","        # put -100 as labels for subwords\n","        # these instances will be ignored during loss calculation\n","        labels = [[self.tag2idx[tag] for tag in doc] for doc in self.tags]\n","        encoded_labels = []\n","        for doc_labels, doc_offset in zip(labels, self.encodings.offset_mapping):\n","            doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100  # empty array with -100\n","            # replace labels of the first subwords with the actual labels\n","            arr_offset = np.array(doc_offset)\n","            doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels\n","            encoded_labels.append(doc_enc_labels.tolist())\n","        return encoded_labels\n","\n","    @staticmethod\n","    def read_input(filepath, tag_only=False):\n","        # tag_only determines whether the labels should include location (B, I)\n","        # or just the tag names (ASPECT, SENTIMENT)\n","        path = Path(filepath)\n","        raw_text = path.read_text().strip()\n","        raw_docs = re.split(r\"\\n\\n\", raw_text)\n","        token_docs = []\n","        tag_docs = []\n","        for doc in raw_docs:\n","            tokens = []\n","            tags = []\n","            for line in doc.split(\"\\n\"):\n","                token, tag = line.strip().split(\"\\t\")\n","                tokens.append(token)\n","                if tag_only:\n","                    tag = tag.split(\"-\")[1] if tag != \"O\" else tag\n","                tags.append(tag)\n","            token_docs.append(tokens)\n","            tag_docs.append(tags)\n","        return token_docs, tag_docs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lWyQU2i_zqHx"},"source":["# Model\n","\n","The `AspectModel` is a wrapper class for the `BertForTokenClassification` model class from ðŸ¤—. There is nothing spectacular happening. And that is a good thing. Fine-tuning pretrained model is just as easy as specifying the model parent class and name.\n","\n","Additionally, I do not need to write my own training loop. ðŸ¤— also provides `Trainer` class which cover all the behaviors that I intended to use. Finally, I add F1 score calculation at every evaluation as extended by the `compute_metrics` function."]},{"cell_type":"code","metadata":{"id":"Xvxxi9J_zrYb"},"source":["class AspectModel(object):\n","\n","    def __init__(self, model_reference, tokenizer, device, num_labels=None, cache_dir=None):\n","        model_args = {\"pretrained_model_name_or_path\": model_reference}\n","        if num_labels is not None:\n","            model_args[\"num_labels\"] = num_labels\n","        if cache_dir is not None:\n","            model_args[\"cache_dir\"] = cache_dir\n","        self.model = BertForTokenClassification.from_pretrained(**model_args)\n","        self.tokenizer = tokenizer\n","        self.device = device\n","        self.trainer = None\n","        self.idx2tag = {}\n","\n","    def train(self,\n","              train_dataset,\n","              val_dataset,\n","              logging_dir,\n","              num_train_epochs,\n","              logging_steps,\n","              lr,\n","              weight_decay,\n","              warmup_steps,\n","              output_dir,\n","              save_model=False\n","              ):\n","        args = TrainingArguments(\n","            output_dir=output_dir,\n","            logging_dir=logging_dir,\n","            num_train_epochs=num_train_epochs,\n","            eval_steps=logging_steps,  # eval and log at the same step frequency\n","            logging_steps=logging_steps,\n","            learning_rate=lr,\n","            weight_decay=weight_decay,\n","            warmup_steps=warmup_steps,  # learning rate keeps increasing up to this point\n","            evaluate_during_training=True,\n","            per_device_train_batch_size=32,\n","            per_device_eval_batch_size=64,\n","        )\n","        self.trainer = Trainer(\n","            model=self.model,\n","            args=args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            compute_metrics=AspectModel.compute_metrics(train_dataset.idx2tag)  # F1 score\n","        )\n","        train_result = self.trainer.train()\n","        if save_model: \n","            self.trainer.save_model(output_dir)\n","            with open(os.path.join(output_dir, \"idx2tag.csv\"), \"w\") as idx2tag_f:\n","                w = csv.writer(idx2tag_f)\n","                w.writerows(train_dataset.idx2tag.items())\n","        return train_result\n","\n","    def predict(self, predict_dataset):\n","        if self.trainer is None:\n","            print(\"Run train() before making prediction.\")\n","            return None\n","        return self.trainer.predict(predict_dataset)\n","\n","    def infer(self, sentence):\n","        tokens = [word_tokenize(sentence)]\n","        encoding = self.tokenizer(\n","            tokens,\n","            return_tensors=\"pt\",\n","            is_pretokenized=True,\n","            padding=True,\n","            truncation=True\n","        )\n","        input_ids = encoding[\"input_ids\"]\n","        attention_mask = encoding[\"attention_mask\"]\n","        subwords = self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n","        input_ids = input_ids.to(self.device)\n","        attention_mask = attention_mask.to(self.device)\n","        outputs = self.model(input_ids, attention_mask=attention_mask)\n","        tags = outputs[0][0].argmax(-1).tolist()\n","        # join subwords\n","        words = []\n","        valid_tags = []\n","        buffer_word = None\n","        buffer_tag = None\n","        for i, (subword, tag) in enumerate(zip(subwords, tags)):\n","            if buffer_word is None:\n","                buffer_word = subword\n","                buffer_tag = tag\n","            elif subword.startswith(\"##\"):\n","                buffer_word += subword.replace(\"##\", \"\")\n","                if i == len(subwords) - 1:\n","                    words.append(buffer_word)\n","            else:\n","                words.append(buffer_word)\n","                valid_tags.append(buffer_tag)\n","                buffer_word = subword\n","                buffer_tag = tag\n","        return words, valid_tags\n","\n","    @staticmethod\n","    def compute_metrics(idx_to_tag):\n","        def _compute_metrics(pred):\n","            valid = pred.label_ids != -100\n","            labels = pred.label_ids[valid].flatten()\n","            preds = pred.predictions.argmax(-1)[valid].flatten()\n","            f1 = f1_score(labels, preds, average=\"micro\", zero_division=0)\n","            report = classification_report(labels, preds, output_dict=True, zero_division=0)\n","            metrics = {\"f1\": f1}\n","            for label in report:\n","                try:\n","                    int_label = int(label)\n","                    if int_label in idx_to_tag:\n","                        metrics[f\"f1_{idx_to_tag[int_label]}\"] = report[label][\"f1-score\"]\n","                except ValueError as _:\n","                    pass\n","            return metrics\n","        return _compute_metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19Ot0BUnzwQ6"},"source":["# Main\n","\n","After defining the custom dataset and model wrapper classes, now we can write the main function. First, we load the pretrained tokenizer `BertTokenizerFast`. Here, I use the `Fast` version to acquire the offset mapping required during tag encodings. The train, val, and test set has been done beforehand, which correspond to 3000, 1000, and 1000 sentences, respectively."]},{"cell_type":"code","metadata":{"id":"isXPt911ihvu"},"source":["model_name = \"bert-base-multilingual-uncased\"\n","tokenizer = BertTokenizerFast.from_pretrained(model_name, cache_dir=f\"{DRIVE_ROOT}/pt_model/\")\n","train_dataset = ReviewDataset(f\"{DRIVE_ROOT}/data/input/train.tsv\", tokenizer)\n","val_dataset = ReviewDataset(f\"{DRIVE_ROOT}/data/input/val.tsv\", tokenizer, tag2idx=train_dataset.tag2idx)\n","test_dataset = ReviewDataset(f\"{DRIVE_ROOT}/data/input/test.tsv\", tokenizer, tag2idx=train_dataset.tag2idx)\n","aspect_model = AspectModel(\n","    model_reference=model_name,\n","    tokenizer=tokenizer,\n","    device=use_device,\n","    num_labels=train_dataset.num_labels,\n","    cache_dir=f\"{DRIVE_ROOT}/pt_model/\"\n",")\n","aspect_model.train(\n","    train_dataset=train_dataset,\n","    val_dataset=val_dataset,\n","    logging_dir=f\"{DRIVE_ROOT}/logs/indo-absa-hotel\",\n","    num_train_epochs=2,\n","    logging_steps=24,\n","    lr=1e-4,\n","    weight_decay=1e-2,\n","    warmup_steps=94,\n","    output_dir=f\"{DRIVE_ROOT}/results\",\n","    save_model=True  # not saving the best model to save space, change to True otherwise\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Bf5VPmWTw4O"},"source":["## Results\n","\n","The advantages does not stop in training only. After the training is done, we can use tensorboard to view the results because ðŸ¤— `Trainer` writes logs in tensorboard readable format by default. At the very least, this approach helps standardize model performance graphs (Bye bye plotting manually). \n","\n","You can interact with the results by yourself too. If you cannot see the tensorboard below, try to use a different browser (I cannot see the board with Firefox, but Safari displays it with no problem)."]},{"cell_type":"code","metadata":{"id":"fvon2GPZlaZd","colab":{"base_uri":"https://localhost:8080/","height":856},"outputId":"91f1a03c-43c6-45cc-ca19-e55283dbfaec","executionInfo":{"status":"ok","timestamp":1657499639298,"user_tz":420,"elapsed":3683,"user":{"displayName":"cun developer","userId":"05066405186273252932"}}},"source":["%load_ext tensorboard\n","%tensorboard --logdir \"/content/gdrive/MyDrive/Colab Notebooks/Thesis/Absa/logs/indo-absa-hotel\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6007, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"BsUAQKozF1a9","colab":{"base_uri":"https://localhost:8080/","height":174,"referenced_widgets":["7f5d5b2f935f4d8f97264486cc373b87","123efcf00bfc44539a96aa7334f01261","64ee34cc50ed4d39a2b971d412b37d7c","02684394f3e24cf6903568c90f48b143","c7d7cccbf0704da499ef5c14cabe6e6e","40ec8507f67545d3a28822babfaa5b3f","0645d6614f2b4e1daf6c0d20527265fd","3adbd3567936459cba25549f2585bd06","406ae25ca3d4448592d8862caf82227e","2d3fc48319054a16a613cae52f4cdb8f","6012c5ec4ef3422db1438995281d757f"]},"outputId":"ba8bd3e9-7152-48f8-c2e6-43e8a7d346c7","executionInfo":{"status":"ok","timestamp":1657506969932,"user_tz":420,"elapsed":3287,"user":{"displayName":"cun developer","userId":"05066405186273252932"}}},"source":["from pprint import pprint\n","# test set final model\n","test_pred = aspect_model.predict(test_dataset)\n","pprint(test_pred[2])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Prediction:   0%|          | 0/16 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5d5b2f935f4d8f97264486cc373b87"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_f1': 0.9369596076913151,\n"," 'eval_f1_B-ASPECT': 0.9129464285714286,\n"," 'eval_f1_B-SENTIMENT': 0.9305123418377931,\n"," 'eval_f1_I-ASPECT': 0.8743633276740238,\n"," 'eval_f1_I-SENTIMENT': 0.878186968838527,\n"," 'eval_f1_O': 0.9533039647577094,\n"," 'eval_loss': 0.17906426778063178}\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_excel('/content/gdrive/MyDrive/Colab Notebooks/Thesis/Centroid/1.xlsx')"],"metadata":{"id":"qbd60SWyrhYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrM0szzu0K1a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa0ab2f6-a1f4-41e6-c284-c0d26b818322","executionInfo":{"status":"ok","timestamp":1657507206450,"user_tz":420,"elapsed":365,"user":{"displayName":"cun developer","userId":"05066405186273252932"}}},"source":["# text = \"tempatnya sempurna, pas didepan pantai losari, namun sayang acnya kurang dingin padahal sdh lapor namun tetep tidak bisa dingin.\"\n","tokens, pred_tags = aspect_model.infer(df['ulasan'][50])\n","max_len = max([len(token) for token in tokens])\n","for token, pred_tag in zip(tokens, pred_tags):\n","    print(f\"{token.ljust(max_len)}\\t{train_dataset.idx2tag[pred_tag]}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]   \tO\n","lokasi  \tB-ASPECT\n","bagus   \tB-SENTIMENT\n","ada     \tB-SENTIMENT\n","makanan \tB-ASPECT\n","enak    \tB-SENTIMENT\n","di      \tO\n","sekitar \tO\n",".       \tO\n","hangat  \tO\n","&       \tO\n","amp     \tO\n",";       \tO\n","membantu\tO\n","staf    \tO\n",".       \tO\n"]}]},{"cell_type":"code","source":["df['ulasan'][50]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3MhwM8gs-mjJ","executionInfo":{"status":"ok","timestamp":1657507209154,"user_tz":420,"elapsed":348,"user":{"displayName":"cun developer","userId":"05066405186273252932"}},"outputId":"99262765-f09c-4a7a-c613-0d2dd547223e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Lokasi bagus ada makanan enak di sekitar. Hangat &amp; membantu staf.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["def extract_as(text):\n","  tokens, pred_tags = aspect_model.infer(text)\n","  aspect = []\n","  index = 0\n","\n","  for token, pred_tag in zip(tokens, pred_tags):\n","    if(\"B-ASPECT\" in train_dataset.idx2tag[pred_tag]):\n","      aspect.append(token)\n","      index+=1\n","    elif(\"I-ASPECT\" in train_dataset.idx2tag[pred_tag]):\n","      if(index == 0):\n","        continue\n","      else:\n","        aspect[index-1]+=\" \"+token\n","    elif(\"SENTIMENT\" in train_dataset.idx2tag[pred_tag]):\n","      if(index == 0):\n","        continue\n","      else:\n","        aspect[index-1]+=\" \"+token\n","    else:\n","      continue\n","    \n","  return aspect"],"metadata":{"id":"3M_83869wTrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_as = []\n","for i in df['ulasan']:\n","  df_as.append(extract_as(i))"],"metadata":{"id":"YZ_iwbRv0lKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['frasa'] = df_as"],"metadata":{"id":"KJznOn_YKA-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":746},"id":"Ak6X3u9FKROx","executionInfo":{"status":"ok","timestamp":1657509516545,"user_tz":420,"elapsed":595,"user":{"displayName":"cun developer","userId":"05066405186273252932"}},"outputId":"70bd5b00-692d-42c4-a80f-16c505ebe280"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              nama  rating    tanggal  \\\n","0           Aan Z.     8.5 2018-09-05   \n","1          Abby P.     8.5 2018-10-29   \n","2      Abdul H. R.     6.1 2020-03-10   \n","3       Abigail C.     9.4 2020-03-16   \n","4      ABIRA M. G.     8.7 2018-07-25   \n","..             ...     ...        ...   \n","510  Grieshelda N.     5.6 2019-06-09   \n","511       Grissela     9.0 2017-09-24   \n","512   Guest-8sg3uu     6.3 2020-02-12   \n","513   Guest-cvsznm     8.5 2019-03-04   \n","514   Guest-fujpsi     6.0 2019-03-30   \n","\n","                                                ulasan  \\\n","0    Lumayan menyenangkan, ambil pool access, kolam...   \n","1    Saya memiliki momen paling menyenangkan mengin...   \n","2    Kamar berbau kamar mandi bau air kamar mandi k...   \n","3           Layanan hebat, kamar sepadan dengan harga.   \n","4    Kotor, saya menghabiskan 4 malam di alea dan m...   \n","..                                                 ...   \n","510  Kamar gelap tidak seperti di gambar, sarung da...   \n","511  Secara keseluruhan bagus. Ragam makanan dan ra...   \n","512  Kamar kurang bersih, AC di kamar tidak terasa ...   \n","513  Sangat nyaman menginap di hotel ini karena pel...   \n","514  Saya memesan untuk satu malam saja. Saya membe...   \n","\n","                                             ringkasan  \\\n","0    Kolam renang kecil, kunci pintu sempat tidak b...   \n","1     Hotel nyaman, sangat dekat ke pantai dan makanan   \n","2                                Kamar bau dan berdebu   \n","3                 Layanan hebat dan kamar sesuai harga   \n","4           Sprei dan bantal kotor. Koneksi wifi buruk   \n","..                                                 ...   \n","510  Kamar gelap dan fasilitas kamar kotor untuk sa...   \n","511        Makanan dibuat beragam dan peningkatan rasa   \n","512     Kamar kurang bersih dan AC tidak terasa dingin   \n","513  Hotel sangat nyaman dengan pelayanan dan fasil...   \n","514  Kamar tidak terawat seperti sprei yang ada nod...   \n","\n","                                                 frasa  \n","0    [ambil pool access, kolam renang kecil, kunci ...  \n","1                                                   []  \n","2    [kamar berbau, kamar mandi bau, air kamar mand...  \n","3                       [layanan hebat, kamar sepadan]  \n","4    [seprai, bantal sangat mengecewakan, koneksi, ...  \n","..                                                 ...  \n","510  [kamar gelap, sarung, sprei bau tidak bersih, ...  \n","511  [secara keseluruhan bagus ragam, makanan, rasa...  \n","512      [kamar kurang bersih, ac tidak terasa dingin]  \n","513                    [pelayanan, kamar sangat bagus]  \n","514  [hotel tidak terawat modern, secara keseluruha...  \n","\n","[515 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-0ddc3fc2-f116-414e-a15d-8d80bd1b545e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>nama</th>\n","      <th>rating</th>\n","      <th>tanggal</th>\n","      <th>ulasan</th>\n","      <th>ringkasan</th>\n","      <th>frasa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Aan Z.</td>\n","      <td>8.5</td>\n","      <td>2018-09-05</td>\n","      <td>Lumayan menyenangkan, ambil pool access, kolam...</td>\n","      <td>Kolam renang kecil, kunci pintu sempat tidak b...</td>\n","      <td>[ambil pool access, kolam renang kecil, kunci ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Abby P.</td>\n","      <td>8.5</td>\n","      <td>2018-10-29</td>\n","      <td>Saya memiliki momen paling menyenangkan mengin...</td>\n","      <td>Hotel nyaman, sangat dekat ke pantai dan makanan</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Abdul H. R.</td>\n","      <td>6.1</td>\n","      <td>2020-03-10</td>\n","      <td>Kamar berbau kamar mandi bau air kamar mandi k...</td>\n","      <td>Kamar bau dan berdebu</td>\n","      <td>[kamar berbau, kamar mandi bau, air kamar mand...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Abigail C.</td>\n","      <td>9.4</td>\n","      <td>2020-03-16</td>\n","      <td>Layanan hebat, kamar sepadan dengan harga.</td>\n","      <td>Layanan hebat dan kamar sesuai harga</td>\n","      <td>[layanan hebat, kamar sepadan]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ABIRA M. G.</td>\n","      <td>8.7</td>\n","      <td>2018-07-25</td>\n","      <td>Kotor, saya menghabiskan 4 malam di alea dan m...</td>\n","      <td>Sprei dan bantal kotor. Koneksi wifi buruk</td>\n","      <td>[seprai, bantal sangat mengecewakan, koneksi, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>510</th>\n","      <td>Grieshelda N.</td>\n","      <td>5.6</td>\n","      <td>2019-06-09</td>\n","      <td>Kamar gelap tidak seperti di gambar, sarung da...</td>\n","      <td>Kamar gelap dan fasilitas kamar kotor untuk sa...</td>\n","      <td>[kamar gelap, sarung, sprei bau tidak bersih, ...</td>\n","    </tr>\n","    <tr>\n","      <th>511</th>\n","      <td>Grissela</td>\n","      <td>9.0</td>\n","      <td>2017-09-24</td>\n","      <td>Secara keseluruhan bagus. Ragam makanan dan ra...</td>\n","      <td>Makanan dibuat beragam dan peningkatan rasa</td>\n","      <td>[secara keseluruhan bagus ragam, makanan, rasa...</td>\n","    </tr>\n","    <tr>\n","      <th>512</th>\n","      <td>Guest-8sg3uu</td>\n","      <td>6.3</td>\n","      <td>2020-02-12</td>\n","      <td>Kamar kurang bersih, AC di kamar tidak terasa ...</td>\n","      <td>Kamar kurang bersih dan AC tidak terasa dingin</td>\n","      <td>[kamar kurang bersih, ac tidak terasa dingin]</td>\n","    </tr>\n","    <tr>\n","      <th>513</th>\n","      <td>Guest-cvsznm</td>\n","      <td>8.5</td>\n","      <td>2019-03-04</td>\n","      <td>Sangat nyaman menginap di hotel ini karena pel...</td>\n","      <td>Hotel sangat nyaman dengan pelayanan dan fasil...</td>\n","      <td>[pelayanan, kamar sangat bagus]</td>\n","    </tr>\n","    <tr>\n","      <th>514</th>\n","      <td>Guest-fujpsi</td>\n","      <td>6.0</td>\n","      <td>2019-03-30</td>\n","      <td>Saya memesan untuk satu malam saja. Saya membe...</td>\n","      <td>Kamar tidak terawat seperti sprei yang ada nod...</td>\n","      <td>[hotel tidak terawat modern, secara keseluruha...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>515 rows Ã— 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ddc3fc2-f116-414e-a15d-8d80bd1b545e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0ddc3fc2-f116-414e-a15d-8d80bd1b545e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0ddc3fc2-f116-414e-a15d-8d80bd1b545e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["df.to_csv(f\"{DRIVE_ROOT}/preprocessing/train.csv\", index=False, encoding='utf-8')"],"metadata":{"id":"ReP01SfiKXtR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXvp1fiTdp_O"},"source":["One interesting detail from the inference example: do you notice an invalid tag sequence in the prediction? \n","\n","And the follow-up questions: How could that happen? Is there any way to prevent that? \n","\n","For now, I will leave those questions unanswered. If you have any idea, [let me know](https://twitter.com/yoseflaw)!"]},{"cell_type":"markdown","metadata":{"id":"9gnRibU9dB85"},"source":["# Conclusion\n","\n","Fine-tuning rocks! We do not have to train from scratch (and potentially waste so much resources). The multilingual BERT models from ðŸ¤— is a good starting point for those who have limited resources and are working in non-English languages. Those models have been trained on a (very) large corpus. Even the multilingual tokenizer works better than expected, which does not make sense to me at the beginning (every language has a unique tokenization method).\n","\n","You can see all the complete list of available models [here](https://huggingface.co/transformers/pretrained_models.html). Spoiler alert: it contains more models than just BERT!\n","\n","Happy fun-tuning!"]}]}